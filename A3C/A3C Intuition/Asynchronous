Asynchronous

강화학습에 대해 다시 살펴보면...
환경과 Agent의 상호작용을 계속 하여 Agent가 특정 행동을 하도록 유도하는 것

같은 조건의 환경과 Agent를 여럿을 배치하여 동시에 실행시키는 것!!! : 병렬 처리
대신 일부 초기 시작 점들은 다 다른 형태로 시작

> 그러면 같은 결과를 얻기 위해 걸리는 시간이 파격적으로 줄어들게 됨
또한 local minimum에 빠져들 위험이 줄어듦
각각 다 다르게 학습하기 때문에 서로 다른 최적의 값을 갖게 됨

보다 다양한 형태의 최적의 형태가 나타나게 됨
그리고 나서 각각 얻어진 Critic을 하나의 Critic으로 갖게 됨
각각의 neural network는 critic 값을 하나로 공유하고 쓰게 됨

A3C는 병렬로 처리하되 하나의 critic value를 공유하는 형태의 강화학습

여기서 하나 더 나아가
neural network를 agent가 공용으로 처리하게 된다면??

Let's Make An A3c: Implementation, Jaromir Janisch (2017)
